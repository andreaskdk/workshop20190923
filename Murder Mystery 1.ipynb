{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Murder Mystery 1 Tamam Shud\n",
    "\n",
    "In December 1948 the dead body of a man was found near Adelaide, Australia. To this day his identity is unknown and his death is still a mystery. One of the only clues recovered to what happened is a small piece of paper with the words \"Tamam Shud\" written on it. The piece has turned out to be torn from the last page of the book \"Rubaiyat\" by Omar Khayyam. The police managed to find the copy of the book from which the piece was torn. This book had some letters written inside the cover:\n",
    "\n",
    "**WRGOABABD**\n",
    "\n",
    "**MLIAOI**\n",
    "\n",
    "**WTBIMPANETP**\n",
    "\n",
    "**MLIABOAIAQC**\n",
    "\n",
    "**ITTMTSAMSTGAB**\n",
    "\n",
    "The second line seems to have been crossed out. The similarity to the penultimate line could suggest that it was a mistake.\n",
    "\n",
    "What does this mean? Is it some kind of code?\n",
    "\n",
    "![Tamam Shud](https://storage.googleapis.com/big-data-course-datasets/Actual-tamam-shud.jpg)\n",
    "\n",
    "![Code](https://storage.googleapis.com/big-data-course-datasets/SomertonManCode.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "code=[\"WRGOABABD\",\n",
    "#\"MLIAOI\",\n",
    "\"WTBIMPANETP\",\n",
    "\"MLIABOAIAQC\",\n",
    "\"ITTMTSAMSTGAB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One theory that we can start by looking into is that the letters is a short hand code for a sentence where only the starting letters of each word is written down. We can test this theory by investigating if distribution of letters is compatible with the general distribution of words in the English language.\n",
    "\n",
    "We first need to find the distribution of starting letters in English, and for that we will use data from the [Gutenberg Project](https://www.gutenberg.org/), which makes avaiable a large number of books in digital form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "books=spark.sparkContext.textFile(\"gs://big-data-course-datasets/gutenberg/\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Spark to find the counts of each starting letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "counts=books.flatMap(lambda x: x.replace(\",\", \"\").split()) \\\n",
    "    .filter(lambda x: len(x)>0) \\\n",
    "    .map(lambda x: x[0].upper()) \\\n",
    "    .filter(lambda x: x in string.ascii_uppercase) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C', 73345),\n",
       " ('N', 48820),\n",
       " ('H', 151054),\n",
       " ('Y', 32791),\n",
       " ('X', 100),\n",
       " ('O', 130771),\n",
       " ('W', 137955),\n",
       " ('S', 162160),\n",
       " ('D', 55619),\n",
       " ('K', 14483),\n",
       " ('J', 13206),\n",
       " ('G', 40746),\n",
       " ('A', 242153),\n",
       " ('V', 12893),\n",
       " ('F', 78982),\n",
       " ('M', 91287),\n",
       " ('R', 34603),\n",
       " ('P', 53167),\n",
       " ('Q', 3571),\n",
       " ('B', 97838),\n",
       " ('T', 347977),\n",
       " ('L', 56497),\n",
       " ('I', 128541),\n",
       " ('E', 41718),\n",
       " ('U', 31154),\n",
       " ('Z', 1058)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can count the letters in the code and compare this to the expected counts if the code is indeed starting letters from English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_frequencies=np.zeros(len(string.ascii_uppercase))\n",
    "for key in dict(counts).keys():\n",
    "    book_frequencies[string.ascii_uppercase.index(key)]+=dict(counts)[key]\n",
    "    \n",
    "book_frequencies=book_frequencies/sum(book_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_frequencies=np.zeros(len(string.ascii_uppercase))\n",
    "for letter in \"\".join(code):\n",
    "    code_frequencies[string.ascii_uppercase.index(letter)]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(data={\"letter\": list(string.ascii_uppercase), \n",
    "                   \"book_freq\": book_frequencies, \n",
    "                   \"code_freq\": code_frequencies})\n",
    "\n",
    "data[\"expected_freq_book\"]=data[\"code_freq\"].sum()*data[\"book_freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.barplot(x=\"letter\", \n",
    "            y=\"val\",\n",
    "            hue=\"frequencies\",\n",
    "            data=data[[\"letter\", \"code_freq\", \"expected_freq_book\"]].melt(id_vars=[\"letter\"], var_name=\"frequencies\", value_name=\"val\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when we plot the expected frequencies with the actual frequencies we can see that the distribution is not too far off, but we can actually also test this with a chi-squared test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=30.940002102352494, pvalue=0.1910131742960432)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.chisquare(data[\"code_freq\"], data[\"expected_freq_book\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value of the test is 19% which means that if we have guessed the correct distribution of letters then there is one chance in five that the actual letters would fit the distribution worse.\n",
    "\n",
    "We can compare this to the uniform distribution of letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=4697.0, pvalue=0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.chisquare(data[\"code_freq\"], np.ones(len(data))/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the frequencies of the code fits the starting letters of words in English. But, perhaps, it is not the starting letters, but some mutations of words. How well does the code fit the general usage of letters in the English language?\n",
    "\n",
    "To answer this, redo the calculation of the letter frequencies, but this time take all letters into account, not just the start of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
