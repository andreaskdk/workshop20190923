{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word count\n",
    "\n",
    "Word count has been called the \"Hello World\" of big data. This makes sense because counting occurences of words comes down one **map** and one **reduce** operation. It turns out that suprisingly many of the more complex types of analysis we wish to perform on data can be broken down to operations very similar to word count.\n",
    "\n",
    "Let us start with a small dataset of Shakespear quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_data = spark.sparkContext.parallelize([\"all that glitters is not gold\", \n",
    "                                     \"love all, trust a few, do wrong to none\",\n",
    "                                    \"to be or not to be that is the question\",\n",
    "                                    \"my kingdom for a horse\",\n",
    "                                    \"the world is a stage\",\n",
    "                                    \"is this a dagger that I see before me\",\n",
    "                                    \"nothing will come of nothing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark we can perform wordcount in very few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('glitters', 1),\n",
       " ('is', 4),\n",
       " ('gold', 1),\n",
       " ('love', 1),\n",
       " ('do', 1),\n",
       " ('none', 1),\n",
       " ('question', 1),\n",
       " ('world', 1),\n",
       " ('stage', 1),\n",
       " ('this', 1),\n",
       " ('before', 1),\n",
       " ('of', 1),\n",
       " ('all', 1),\n",
       " ('that', 3),\n",
       " ('not', 2),\n",
       " ('all,', 1),\n",
       " ('trust', 1),\n",
       " ('a', 4),\n",
       " ('few,', 1),\n",
       " ('wrong', 1),\n",
       " ('to', 3),\n",
       " ('be', 2),\n",
       " ('or', 1),\n",
       " ('the', 2),\n",
       " ('my', 1),\n",
       " ('kingdom', 1),\n",
       " ('for', 1),\n",
       " ('horse', 1),\n",
       " ('dagger', 1),\n",
       " ('I', 1),\n",
       " ('see', 1),\n",
       " ('me', 1),\n",
       " ('nothing', 2),\n",
       " ('will', 1),\n",
       " ('come', 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_data.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b) \\\n",
    "             .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this operation by operation:\n",
    "\n",
    "- **flatMap(lambda line: line.split(\" \"))**: The input to this is an RDD of quote-lines. This RDD has length 7, because there are 7 quotes. These lines are then split into words and outputted as an RDD of individual words. The output RDD will have length 49, because this is the number of individual words. Had we used **map()** instead of **flatMap()** we wold have had an output RDD of length 7, where each element would have been a list of words. This is the difference between **flatMap()** and **map()**.\n",
    "- **map(lambda word: (word, 1))**: For each of the 49 input words we turn them into tuples of the word and an integer of 1. This we do to prepare for the next operation which is reduce.\n",
    "- **reduceByKey(lambda a, b: a + b)**: The input tuples are interpreted as (key, value)-pairs and all the values of the same key are subjected to the addition operation. We do not know the order of the values and operation, so it is important to choose a reduce operation such that the order in which the values are processed is insignificant. Formally, this means that the operation must be commutative and associative (see https://en.wikipedia.org/wiki/Commutative_property and https://en.wikipedia.org/wiki/Associative_property )\n",
    "- **collect()**: All the previous operations have been performed in parallel on the nodes in the cluster. We now have a dataset small enough for us to collect it to the client, which in our case is the master-node of the Spark-cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a few more operations to sort the list of words by occurance and only retrieve the top ten words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 4),\n",
       " ('a', 4),\n",
       " ('that', 3),\n",
       " ('to', 3),\n",
       " ('not', 2),\n",
       " ('be', 2),\n",
       " ('the', 2),\n",
       " ('nothing', 2),\n",
       " ('glitters', 1),\n",
       " ('gold', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_data.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b) \\\n",
    "             .map(lambda x: (x[1], x)) \\\n",
    "             .sortByKey(False) \\\n",
    "             .map(lambda x: x[1]) \\\n",
    "             .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also explain the additional operations:\n",
    "\n",
    "- **map(lambda x: (x[1], x))**: The input of this **map**-operation is tuples of words and their counts (word, count). The output is tuples of counts and the input tuple (count, (word, count)).\n",
    "- **sortByKey(False)**: As the name suggest we sort the input tuples by key. The boolean argument determines if we are sorting in ascending or descending order. In this case we are interested in the most common words and we therefore sort in descending order.\n",
    "- **map(lambda x: x[1])**: The previous **sort**-operation kept the keys, so we do a **map** to remove the keys and just keep the (word, count)-tuples.\n",
    "- **take(10)**: We collect the 10 first objects of the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------\n",
    "\n",
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seven Shakespeare quotes are hardly \"big data\", so let us try a dataset which is a little larger. Below we load a dataset consisting of all the New Year's speeches of the queen of Denmark from 1972 to 2018.\n",
    "\n",
    "Try to find the top 10 words used in all of these.\n",
    "\n",
    "What are the counts of the words \"prins\" (prince) and \"prinsesse\" (princess)? \n",
    "\n",
    "For that last question it may be usefull to look at the **filter()** function ( https://spark.apache.org/docs/2.2.0/api/python/pyspark.html#pyspark.RDD )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nytaar = spark.sparkContext.textFile(\"gs://big-data-course-datasets/nytaar/\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
